{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code\n",
    "\n",
    "At some points we could not make the rdd.map function work so we switched to regular map. Unfortunately then we don't really use pyspark's efficiency but at least we can answer the research questions.\n",
    "\n",
    "\n",
    "#### HDFS\n",
    "Could be used using: \n",
    "\n",
    "data = sc.textFile('hdfs://scomp1334:9000/user/steen176/csv/butterflies.csv')\n",
    "\n",
    "\n",
    "#### addPyFile\n",
    "Adds the classes to store species and observations to the workers\n",
    "\n",
    "sc.addPyFile(\"classes.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from classes import Species, Observation\n",
    "import subprocess as sp\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global species\n",
    "    \n",
    "    #This adds the classes.py to the pyspark context so it will be available for the worker nodes\n",
    "    sc.addPyFile(\"classes.py\")\n",
    "\n",
    "    data = sc.textFile(\"/home/WUR/steen176/Downloads/butterflies.csv\")\n",
    "    corrected = data.map(lambda line: correctLine(line))\n",
    "    species = extractSpecies(corrected)\n",
    "    \n",
    "    \n",
    "    #We take the corrected dataset and parse each species+observations\n",
    "    #rdd.map did not work here\n",
    "    map(getObservation, corrected.toLocalIterator())\n",
    "    \n",
    "    #rdd.map did not work here\n",
    "    clusters = map(cluster, species.keys())\n",
    "    \n",
    "    #GetList of rare species. Change top to another value to increase the number of rare species added\n",
    "    top = 1\n",
    "    rare_specs =  sorted([(specie, len(species[specie].observations)) for specie in species.keys()], \n",
    "                         key = lambda x: x[1])[0:top]\n",
    "    \n",
    "    \n",
    "    #currently only for the rarest specie. If you want to do this for more than 1 species you need to loop over\n",
    "    # rare_specs and do everything below here (until the end of the cell) in that loop (except for the subprocess line).\n",
    "    # rare_specs[i] is each species that is rare\n",
    "    # rare_specs[i][0] is the name of the rare species\n",
    "    # rare_specs[i][1] is the KmeansCluster Object\n",
    "    rarest_name = rare_specs[0][0]\n",
    "    rarest_cluster = cluster(rarest_name)[1].clusterCenters\n",
    "    rarest = (rarest_name, cluster(rarest_name)[1].clusterCenters)\n",
    "    \n",
    "    #Removes the rare_specie from the list with all cluster means, so we can compare the rare one with the rest without\n",
    "    #Comparing the rare specie with itself.\n",
    "    #The resulting list contains tuples as  values in the format of (speciesName, [center1,center2, etc.]])\n",
    "    filtered_clusters = [(x[0], [x[1].clusterCenters]) for x in clusters if \n",
    "                       [tuple(y.tolist()) for y in x[1].clusterCenters] != \n",
    "                       [tuple(y.tolist()) for y in rarest_cluster]\n",
    "                        ]\n",
    "   \n",
    "    \n",
    "    \n",
    "    #output appends to the result file out.tsv in the sp.call below the results are filtered and writen to the real output\n",
    "    # file called output.tsv (out.tsv is then deleted)\n",
    "    with open(\"out.tsv\", \"a\") as out_file:\n",
    "        for x in findCombinations(rarest, filtered_clusters):\n",
    "            out_file.write(x)\n",
    "            \n",
    "    \n",
    "    #Removes duplicates that could be create in the findCombinations function\n",
    "    sp.call(\"echo 'rare_species\\tclusters_with\\tlongtitude\\tlattitude' > output.tsv; sort -u out.tsv >> output.tsv;rm out.tsv\", shell = True)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSpecies(line):\n",
    "    \"\"\"Returns the species as string for the entered line\"\"\"\n",
    "    line = line.split(',')\n",
    "    return line[0]\n",
    "\n",
    "def correctLine(line):\n",
    "    \"\"\"Returns the line with fixed colums.\n",
    "    Sometimes people that use database enter a comma in a field and then when the line is being.\n",
    "    Split on commas there will be a column to many. This method fixes that error.\n",
    "    \"\"\"\n",
    "    line = line.split(',')\n",
    "    if len(line) != 10:\n",
    "        return ','.join(line[0:4] + [\" \".join(line[4:6])]+ line[6:])\n",
    "    else:\n",
    "        return ','.join(line)\n",
    "\n",
    "def getObservation(line):\n",
    "    \"\"\"Extract observations from a line.\n",
    "    \n",
    "        Keyword Arguments:\n",
    "            line -- Comma seperated observation line.\n",
    "            \n",
    "    A line can contain multiple observations from the same species. If multiple are present\n",
    "    Observations are created based on the corresponding numbers. The created Observation\n",
    "    objects are added to the corresponding species object.    \n",
    "    \"\"\"\n",
    "    line = line.split(',')\n",
    "    specie = line[0]\n",
    "    for i in xrange(int(line[5])):  #line[5] is the number of observations\n",
    "        longti = line[8]\n",
    "        lati = line[9]\n",
    "        obs = Observation(longti,lati)\n",
    "        species[specie].addObservation(obs)\n",
    "    \n",
    "    \n",
    "def createSpecieObjects(species_list):\n",
    "    \"\"\"Create a dictionary with all species from the database in the format {species_name: species_object}.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            species_list -- A list containing all individual specie names.\n",
    "        Returns:\n",
    "            species_objects -- A dictionary containing species and species Objects\n",
    "    \"\"\"\n",
    "    species_objects = {}\n",
    "    for specie in species_list:\n",
    "        obj = Species(specie)\n",
    "        species_objects[specie] = obj\n",
    "    return species_objects\n",
    "\n",
    "\n",
    "def extractSpecies(rdd):\n",
    "    species = rdd.map(getSpecies)\n",
    "    species_list  = list(set([x for x in species.toLocalIterator()]))\n",
    "    return createSpecieObjects(species_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from math import sqrt\n",
    "NUM_CLUSTER = 4\n",
    "\n",
    "def kmeans(sparse_vectors):\n",
    "    \"\"\"Clusters the sample using k-means with a random initalization mode\n",
    "    \n",
    "        Keyword Arguments:\n",
    "            sparse_vectors -- An RDD object containing \n",
    "        Returns:\n",
    "            The Kmeans object\n",
    "    \"\"\"\n",
    "    return KMeans.train(sparse_vectors, NUM_CLUSTER, maxIterations=20, runs=100, initializationMode=\"random\")\n",
    "\n",
    "def cluster(specie):\n",
    "    \"\"\"Prepare the samples for clustering and call the clustering function\n",
    "    \n",
    "        Keyword Arguments:\n",
    "            specie -- The name of the specie to be clustered\n",
    "        Returns:\n",
    "            A tuple of (specie_name, KmeansObject)\n",
    "            \n",
    "        The species name has to be provided, next the species object is extracted from\n",
    "        the species dictionary. the species object is then used to get an RDD containing SparseVectors \n",
    "        of all observations of that species.\n",
    "    \"\"\"\n",
    "    vectors = species[specie].getVectorRDD()\n",
    "    return (specie, kmeans(sc.parallelize(vectors)))\n",
    "\n",
    "\n",
    "    \n",
    "def error(point):\n",
    "    \"\"\" Was in the example code, not used in this code\"\"\"\n",
    "    \n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    denseCenter = DenseVector(numpy.ndarray.tolist(center))\n",
    "    return sqrt(sum([x**2 for x in (DenseVector(point.toArray()) - denseCenter)]))\n",
    "\n",
    "    WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(WSSSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy \n",
    "def centerdiffs(centerA, centerB, DIFF=0.1):\n",
    "    \"\"\"Calculates the difference between longtitude and lattitude\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            centerA -- A numpy array of the longtitude and lattitude as floats\n",
    "            centerB -- A numpy array of the longtitude and lattitude as floats\n",
    "        Returns:\n",
    "            True if the Differences are between -DIFF and DIFF\n",
    "            False if the Difference are not between -DIFF and DIFF\n",
    "            \n",
    "        If the difference is smaller than DIFF the cluster centers are overlapping\n",
    "    \"\"\"\n",
    "    longA = centerA[0]\n",
    "    longB = centerB[0]\n",
    "    lattA = centerA[1]\n",
    "    lattB = centerB[1]\n",
    "    \n",
    "    longDiff = numpy.subtract(longA, longB)\n",
    "    lattDiff = numpy.subtract(lattA, lattB)\n",
    "    \n",
    "    #print(\"Longdiff: {}\\t\\tLattdiff: {}\".format(str(longDiff), str(lattDiff)))\n",
    "\n",
    "    if (longDiff < DIFF and longDiff > DIFF*-1) and (lattDiff < DIFF and lattDiff > DIFF*-1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def findCombinations(rare_specie, all_species_centers):\n",
    "    \"\"\"Compare a rare specie with all other species and see if some clusters overlap.\n",
    "    \n",
    "        Keyword Arguments:\n",
    "            rare_specie_centers -- The np.array of cluster centers\n",
    "                    format: (u'Cuculus canorus', Array())\n",
    "            all_species_centers -- The np.array of cluster centers for all other species\n",
    "        Yields:\n",
    "            All overlapping clusters with name and location, ready for writing to the output file\n",
    "    \"\"\"\n",
    "    rare_name = rare_specie[0]\n",
    "    rare_centers = rare_specie[1]\n",
    "    for one in all_species_centers:\n",
    "        other_name = one[0]\n",
    "        for center in rare_centers:\n",
    "            for one_center in one[1][0]:\n",
    "                if centerdiffs(center, one_center):\n",
    "                    yield \"{}\\t{}\\t{}\\t{}\\n\".format(rare_name, other_name, str(center[0]), str(center[1]))\n",
    "                else:\n",
    "                    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
